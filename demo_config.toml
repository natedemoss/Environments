# Demo Configuration for Standardized Environment Evaluation
# This shows how to configure multi-environment benchmarks with TOML

[metadata]
name = "Multi-Environment Benchmark Demo"
description = "Evaluate ReAct agents across multiple environments"
version = "1.0"
author = "Synth Team"

[agent]
type = "ReActAgent"
model_name = "gemini-1.5-flash"
temperature = 0.1
max_tokens = 1000

[evaluation]
# List of environments to evaluate
environments = ["Sokoban", "CrafterClassic", "NetHack", "MiniGrid", "TicTacToe"]
episodes_per_env = 5
parallel_environments = 2
timeout_seconds = 300
save_intermediate_states = true

# Environment-specific configurations
[environment_configs]

[environment_configs.Sokoban]
difficulty = "medium"
max_steps = 100
scenarios = [
    { name = "easy", difficulty = "easy", max_steps = 50, num_episodes = 5 },
    { name = "medium", difficulty = "medium", max_steps = 100, num_episodes = 3 },
    { name = "hard", difficulty = "hard", max_steps = 150, num_episodes = 2 }
]

[environment_configs.CrafterClassic]
max_steps = 1000
target_achievements = 5
scenarios = [
    { name = "survival", max_steps = 500, num_episodes = 3 },
    { name = "exploration", max_steps = 1000, num_episodes = 2 }
]

[environment_configs.NetHack]
character_role = "tourist"
target_depth = 3
max_steps = 500
scenarios = [
    { name = "tutorial", target_depth = 2, max_steps = 300, num_episodes = 3 },
    { name = "intermediate", target_depth = 3, max_steps = 500, num_episodes = 2 }
]

[environment_configs.MiniGrid]
env_type = "DoorKey-6x6-v0"
max_steps = 200
scenarios = [
    { name = "simple", max_steps = 100, num_episodes = 5 },
    { name = "complex", max_steps = 200, num_episodes = 3 }
]

[environment_configs.TicTacToe]
agent_plays_first = true
scenarios = [
    { name = "standard", num_episodes = 10 }
]

# Metrics and reporting configuration
[metrics]
track_intermediate_rewards = true
track_achievements = true
track_step_by_step = false  # Set to true for detailed debugging
save_episode_videos = false  # For environments that support it

[reporting]
output_dir = "results"
generate_plots = true
generate_html_report = true
save_raw_data = true

# Comparison configurations (for running multiple agents/models)
[[comparisons]]
name = "model_comparison"
models = ["gemini-1.5-flash", "gpt-4o-mini", "claude-3-haiku"]
environments = ["Sokoban", "TicTacToe"]
episodes_per_model = 5 